import { Steps } from 'nextra/components'
import { Callout } from 'nextra/components'
import { Tabs } from 'nextra/components'

# Visual Pointing üöß

Moondream can identify and return precise coordinates for objects and regions within images. This capability enables applications like GUI automation, visual instruction following, and interactive tutorials.

## Overview

The pointing capability allows you to:
- Get precise x,y coordinates for objects in images
- Locate specific UI elements or regions
- Enable computer vision agents to interact with interfaces
- Support accessibility features through visual guidance

## Basic Usage

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

model_id = "vikhyatk/moondream2"
model = AutoModelForCausalLM.from_pretrained(
    model_id, trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Load image
image = Image.open('screenshot.png')
enc_image = model.encode_image(image)

# Get coordinates for a specific object
coordinates = model.point_at_object(enc_image, "the submit button", tokenizer)
print(f"Object coordinates: {coordinates}")
```

## Response Format

The model returns coordinates in the following format:

```python
{
    "coordinates": {
        "x": float,  # x-coordinate (0-1)
        "y": float   # y-coordinate (0-1)
    },
    "confidence": float  # confidence score (0-1)
}
```

Coordinates are normalized to the range [0,1] relative to the image dimensions:
- (0,0) represents the top-left corner
- (1,1) represents the bottom-right corner

## Use Cases

### GUI Automation

```python
# Example: Automate clicking a button
coordinates = model.point_at_object(enc_image, "the login button", tokenizer)
click_position = (
    coordinates["x"] * screen_width,
    coordinates["y"] * screen_height
)
pyautogui.click(click_position)
```

### Visual Instruction Following

```python
# Example: Follow a sequence of visual instructions
instructions = [
    "click the menu icon",
    "find the settings button",
    "locate the dark mode toggle"
]

for instruction in instructions:
    coords = model.point_at_object(enc_image, instruction, tokenizer)
    # Perform action at coordinates
```

### Accessibility Support

```python
# Example: Help visually impaired users locate elements
def describe_location(coordinates):
    x, y = coordinates["x"], coordinates["y"]
    
    if x < 0.33:
        h_pos = "left side"
    elif x < 0.66:
        h_pos = "middle"
    else:
        h_pos = "right side"
        
    if y < 0.33:
        v_pos = "top"
    elif y < 0.66:
        v_pos = "center"
    else:
        v_pos = "bottom"
        
    return f"The element is on the {v_pos} {h_pos} of the screen"
```

## Best Practices

1. **Input Quality**
   - Provide clear, high-resolution images
   - Ensure good contrast between elements
   - Avoid overlapping objects when possible

2. **Prompting**
   - Be specific in object descriptions
   - Use unique identifiers when available
   - Consider multiple ways to describe the target

3. **Confidence Handling**
   - Check the confidence score before acting
   - Implement fallback strategies for low confidence
   - Consider multiple attempts with different prompts

4. **Coordinate Processing**
   - Convert normalized coordinates to screen/image pixels
   - Add padding for click actions
   - Validate coordinates are within bounds

## Limitations

- Currently in development (marked with üöß)
- May have reduced accuracy with:
  - Cluttered interfaces
  - Very similar objects
  - Low contrast elements
  - Dynamic content

## Coming Soon

- Multi-point detection
- Region selection
- Bounding box coordinates
- Improved accuracy
- Integration with popular automation frameworks

## Implementation Guide

<Steps>
### 1. Download Model
<Tabs items={['Python (pip)', 'JavaScript', 'TypeScript']}>
  <Tabs.Tab>
    ```python
    import os
    import requests
    import gzip
    from tqdm import tqdm

    # Model URLs dictionary
    MODEL_URLS = {
        'int8': "https://huggingface.co/vikhyatk/moondream2/resolve/client/moondream-latest-int8.bin.gz?download=true",
        'fp16': "https://huggingface.co/vikhyatk/moondream2/resolve/client/moondream-latest-f16.bin.gz?download=true",
        'int4': "https://huggingface.co/vikhyatk/moondream2/resolve/client/moondream-latest-int4.bin.gz?download=true",
        '0.5b-int8': "https://huggingface.co/vikhyatk/moondream2/resolve/onnx/moondream-0_5b-int8.bin.gz?download=true",
        '0.5b-int4': "https://huggingface.co/vikhyatk/moondream2/resolve/onnx/moondream-0_5b-int4.bin.gz?download=true"
    }

    def download_model(variant='0.5b-int8', save_dir="models"):
        """Download and extract model weights using wget-style direct extraction.
        
        Args:
            variant (str): Model variant to download. Options:
                - 'int8': Latest INT8 model (2.09 GB)
                - 'fp16': Latest FP16 model (3.74 GB)
                - 'int4': Latest INT4 model (1.52 GB)
                - '0.5b-int8': Lightweight INT8 model (613 MB)
                - '0.5b-int4': Lightweight INT4 model (479 MB)
            save_dir (str): Directory to save the model
        """
        if variant not in MODEL_URLS:
            raise ValueError(f"Invalid variant. Choose from: {list(MODEL_URLS.keys())}")
        
        os.makedirs(save_dir, exist_ok=True)
        output_path = os.path.join(save_dir, f"moondream-{variant.replace('0.5b-', '0_5b-')}.bin")
        
        # Check if model already exists
        if os.path.exists(output_path):
            print(f"‚úÖ Model already exists at: {output_path}")
            return output_path
        
        # Download and extract in one step
        print(f"üì• Downloading and extracting {variant} model...")
        print("‚ö†Ô∏è Download time varies by model size and connection speed...")
        response = requests.get(MODEL_URLS[variant], stream=True)
        total_size = int(response.headers.get('content-length', 0))
        
        with open(output_path, 'wb') as f_out:
            with gzip.GzipFile(fileobj=response.raw, mode='rb') as f_gz:
                with tqdm(total=total_size, unit='iB', unit_scale=True) as pbar:
                    while True:
                        chunk = f_gz.read(1024*1024)
                        if not chunk:
                            break
                        f_out.write(chunk)
                        pbar.update(len(chunk))
        
        print(f"‚úÖ Model ready at: {output_path}")
        return output_path

    # Choose and download model variant
    # Note: Visual pointing requires the 2B model (int8, fp16, or int4)
    variant = 'int8'  # Using 2B INT8 model for pointing
    model_path = download_model(variant=variant)
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```javascript
    // Coming soon
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```typescript
    // Coming soon
    ```
  </Tabs.Tab>
</Tabs>

### 2. Initialize Model
<Tabs items={['Python (pip)', 'JavaScript', 'TypeScript']}>
  <Tabs.Tab>
    ```python
    import moondream as md
    from PIL import Image

    # Initialize model with path to weights
    model = md.VL(model_path)
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```javascript
    // Coming soon
    import { MoondreamAPI } from 'moondream';
    
    const model = new MoondreamAPI();
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```typescript
    // Coming soon
    import { MoondreamAPI } from 'moondream';
    
    const model = new MoondreamAPI();
    ```
  </Tabs.Tab>
</Tabs>

### 3. Load Image
<Tabs items={['Python (pip)', 'JavaScript', 'TypeScript']}>
  <Tabs.Tab>
    ```python
    # Load and encode image
    image = Image.open("scene.jpg")
    encoded_image = model.encode_image(image)
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```javascript
    // Coming soon
    const image = await loadImage("scene.jpg");
    const encodedImage = await model.encodeImage(image);
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```typescript
    // Coming soon
    const image = await loadImage("scene.jpg");
    const encodedImage = await model.encodeImage(image);
    ```
  </Tabs.Tab>
</Tabs>

### 4. Query Location
<Tabs items={['Python (pip)', 'JavaScript', 'TypeScript']}>
  <Tabs.Tab>
    ```python
    # Ask about a specific location
    response = model.point(image, "Where is the cat looking?")
    print(f"Answer: {response['answer']}")
    print(f"Coordinates: {response['coordinates']}")  # [x, y] normalized between 0 and 1
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```javascript
    // Coming soon
    const response = await model.point(encodedImage, "Where is the cat looking?");
    console.log("Answer:", response.answer);
    console.log("Coordinates:", response.coordinates);  // [x, y] normalized between 0 and 1
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```typescript
    // Coming soon
    const response = await model.point(encodedImage, "Where is the cat looking?");
    console.log("Answer:", response.answer);
    console.log("Coordinates:", response.coordinates);  // [x, y] normalized between 0 and 1
    ```
  </Tabs.Tab>
</Tabs>
</Steps>

For basic usage examples, see the [Getting Started](/moondream-docs/getting-started) guide. 