import Head from 'next/head'
import Recipe from '../components/Recipe';

<Head>
  <title>Recipes - Moondream Documentation</title>
  <meta name="description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  {/* Open Graph */}
  <meta property="og:title" content="Moondream Recipes - Best Practices & Real-World Examples" />
  <meta property="og:description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://docs.moondream.ai/recipes" />
  
  
  {/* Twitter */}
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Moondream Recipes - Best Practices & Real-World Examples" />
  <meta name="twitter:description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  
  
  {/* Schema.org for Google */}
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "TechArticle",
      "headline": "Moondream Vision AI Implementation Recipes",
      "description": "Collection of best practices and real-world examples for Moondream vision AI",
      "articleSection": "Technical Documentation",
      "keywords": "vision AI, implementation patterns, best practices, optimization, examples"
    })}
  </script>
</Head>

# Recipes

Explore real-world examples and implementation patterns using Moondream.


  <Recipe
    title="Interactive Image Analysis Demo"
    description="A Gradio-based web interface for real-time image analysis, featuring visual question answering and object detection with bounding box visualization."
    github="https://github.com/vikhyat/moondream/blob/main/gradio_demo.py"
    demo="https://youtu.be/T7sxvrJLJ14?feature=shared&t=1072"
    tags={['Gradio', 'Interactive UI', 'Object Detection', 'VQA']}
  >
    <Recipe.CodePreview 
      filename="gradio_demo.py"
      sourceUrl="https://github.com/vikhyat/moondream/blob/main/gradio_demo.py"
    >
      {`# Core setup
model_id = "vikhyatk/moondream2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
moondream = Moondream.from_pretrained(model_id).to(device)

# Create the interface

with gr.Blocks() as demo: gr.Markdown("# 🌔 moondream") with gr.Row(): prompt = gr.Textbox(label="Input Prompt", value="Describe this image.") submit = gr.Button("Submit") with
gr.Row(): img = gr.Image(type="pil", label="Upload an Image") with gr.Column(): output = gr.Markdown(label="Response") ann = gr.Image(visible=False, label="Annotated Image")`}

</Recipe.CodePreview>

    <Recipe.Feature icon="⚡️" title="Streaming Responses">
      Get real-time text generation with token-by-token streaming for a more
      interactive experience.
    </Recipe.Feature>

    <Recipe.Feature icon="🎯" title="Automatic Annotations">
      Visualize object detection results with automatically generated bounding
      boxes overlaid on images.
    </Recipe.Feature>

    <Recipe.Feature icon="🔄" title="Flexible Runtime">
      Switch between CPU and GPU inference with automatic device detection
      and optimization.
    </Recipe.Feature>

    <Recipe.Feature icon="🚀" title="Simple Deployment">
      Deploy as a standalone web app or integrate into existing Gradio
      applications with minimal configuration.
    </Recipe.Feature>

  </Recipe>

  ---
    <Recipe
    title="Jupyter Notebook Quick Start"
    description="Run Moondream in a Jupyter notebook or Google Colab environment with Google Drive integration for easy image analysis and experimentation."
    github="https://github.com/vikhyat/moondream/blob/main/examples/jupyter_notebook.ipynb"
    demo="https://colab.research.google.com/drive/1g88_P2Is8_dPqu-vX3D9VI80jJMjycUT?usp=sharing"
    tags={['Jupyter', 'Google Colab', 'Object Detection', 'VQA']}
  >
    <Recipe.CodePreview 
      filename="jupyter_notebook.ipynb"
      sourceUrl="https://colab.research.google.com/drive/1g88_P2Is8_dPqu-vX3D9VI80jJMjycUT?usp=sharing"
    >
      {`# Install and import dependencies
!pip install moondream
import moondream as md
from PIL import Image

# Download model weights
variant = '0.5b-int8'  # Lightweight model for basic usage
model_path = download_model(variant=variant)

# Initialize model
model = md.vl(model_path)

# Load and analyze image
image = Image.open("sample.jpg")
encoded_image = model.encode_image(image)

# Generate caption
caption = model.caption(encoded_image)
print(caption["caption"])

# Ask questions about the image
question = "What colors are prominent in this image?"
answer = model.query(encoded_image, question)
print(answer["answer"])`}
    </Recipe.CodePreview>

    <Recipe.Feature icon="⚡️" title="Multiple Model Options">
      Choose from different model variants optimized for your needs - from lightweight 0.5B models to full 2B versions.
    </Recipe.Feature>

    <Recipe.Feature icon="🎯" title="Google Drive Integration">
      Seamlessly load images and save results using Google Drive integration in Colab notebooks.
    </Recipe.Feature>

    <Recipe.Feature icon="🔄" title="Flexible Runtime Options">
      Run on CPU (Free Tier) or upgrade to GPU runtime for faster inference with T4/V100/A100 options.
    </Recipe.Feature>

    <Recipe.Feature icon="🚀" title="Interactive Development">
      Experiment and iterate quickly with interactive Jupyter notebook environment and real-time output.
    </Recipe.Feature>

  </Recipe>
