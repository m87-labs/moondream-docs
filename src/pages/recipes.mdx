import Head from 'next/head'
import Recipe from '../components/Recipe';

<Head>
  <title>Recipes - Moondream Documentation</title>
  <meta name="description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  {/* Open Graph */}
  <meta property="og:title" content="Moondream Recipes - Best Practices & Real-World Examples" />
  <meta property="og:description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://docs.moondream.ai/recipes" />
  
  
  {/* Twitter */}
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Moondream Recipes - Best Practices & Real-World Examples" />
  <meta name="twitter:description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  
  
  {/* Schema.org for Google */}
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "TechArticle",
      "headline": "Moondream Vision AI Implementation Recipes",
      "description": "Collection of best practices and real-world examples for Moondream vision AI",
      "articleSection": "Technical Documentation",
      "keywords": "vision AI, implementation patterns, best practices, optimization, examples"
    })}
  </script>
</Head>

# Recipes

Explore real-world examples and implementation patterns using Moondream...

  <Recipe
    title="Promptable Content Moderation"
    description="A powerful content moderation tool that uses Moondream 2B to detect and moderate content in videos using natural language prompts."
    github="https://github.com/vikhyat/moondream/tree/main/recipes/promptable-content-moderation"
    tags={['Content Moderation', 'Object Tracking', 'DeepSORT', "Moondream2-2025-01-09"]}
  >
    <Recipe.CodePreview 
      filename="promptable-content-moderation.py"
      sourceUrl="https://github.com/vikhyat/moondream/tree/main/recipes/promptable-content-moderation"
    >
      {`#!/usr/bin/env python3
import cv2, os, subprocess, argparse
from PIL import Image
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, SamModel, SamProcessor
from tqdm import tqdm
import numpy as np
from datetime import datetime
import colorsys
import random
from deep_sort_integration import DeepSORTTracker
from scenedetect import detect, ContentDetector
from functools import lru_cache

# Constants
DEFAULT_TEST_MODE_DURATION = 3  # Process only first 3 seconds in test mode by default

def load_moondream():
    """Load Moondream model and tokenizer."""
    model = AutoModelForCausalLM.from_pretrained(
        "vikhyatk/moondream2", trust_remote_code=True, device_map={"": "cuda"}
    )
    tokenizer = AutoTokenizer.from_pretrained("vikhyatk/moondream2")
    return model, tokenizer

def get_video_properties(video_path):
    """Get basic video properties."""
    video = cv2.VideoCapture(video_path)
    fps = video.get(cv2.CAP_PROP_FPS)
    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
    video.release()
    return {"fps": fps, "frame_count": frame_count, "width": width, "height": height}`}
    </Recipe.CodePreview>

    <Recipe.Feature icon="ðŸŽ¯" title="Natural Language Prompts">
      Define content to moderate using natural language descriptions, leveraging Moondream's vision-language capabilities for flexible content detection.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸ”" title="Intelligent Scene Detection">
      Advanced scene detection with DeepSORT tracking and automatic tracker reset at scene boundaries for improved accuracy.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¨" title="Multiple Visualization Styles">
      Choose between different redaction styles including obfuscated-pixel, bounding boxes, and hitmarkers for detected content.
    </Recipe.Feature>

    <Recipe.Feature icon="âš¡" title="Optimized Processing">
      Grid-based detection for complex scenes, frame-by-frame processing with IoU-based merging, and GPU acceleration support.
    </Recipe.Feature>

  </Recipe>

  ---

  <Recipe
    title="Promptable Video Redaction"
    description="A tool that uses Moondream 2B to detect and redact objects from videos with multiple visualization styles, including censoring, bounding boxes, and hitmarkers."
    github="https://github.com/vikhyat/moondream/blob/main/recipes/promptable-video-redaction"
    tags={['Video Processing', 'Object Detection', 'Redaction', 'Moondream2-2025-01-09']}
  >
    <Recipe.CodePreview 

      filename="promptable-video-redaction.py"
      sourceUrl="https://github.com/vikhyat/moondream/blob/main/recipes/promptable-video-redaction"
    >

      {`import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import cv2
import numpy as np
from PIL import Image
import argparse

def process_video(input_path, detect_prompt, box_style='censor', grid_size=(1,1)):
    """Process video with Moondream2 for object detection and redaction."""
    # Initialize Moondream model
    model_id = "vikhyatk/moondream2"
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Open video
    cap = cv2.VideoCapture(input_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # Process frames
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        # Convert frame to PIL Image for Moondream
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # Process with grid-based detection if enabled
        for row in range(grid_size[0]):
            for col in range(grid_size[1]):
                # Process region and apply visualization
                process_region(frame, pil_image, model, tokenizer, 
                             detect_prompt, box_style, row, col, grid_size)
                
    cap.release()`}
    </Recipe.CodePreview>

    <Recipe.Feature icon="ðŸŽ¥" title="Multiple Visualization Styles">
      Choose between censoring (black boxes), traditional bounding boxes, or Call of Duty style hitmarkers for detected objects.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸ”" title="Grid-based Detection">
      Split frames into customizable grids for improved detection accuracy on complex scenes.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸ’¬" title="Natural Language Detection">
      Specify objects to detect using natural language descriptions, leveraging Moondream's vision-language capabilities.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŒ" title="Web Interface">
      User-friendly web interface built with Gradio for easy video processing and configuration.
    </Recipe.Feature>

  </Recipe>

---

  <Recipe
    title="Gaze Detection Video Processor"
    description="A video processing application that uses Moondream 2 to detect faces and track gaze directions in videos, with real-time visualization of face detections and gaze directions using dynamic visual effects."
    github="https://github.com/vikhyat/moondream/tree/main/recipes/gaze-detection-video"
    tags={['Video Intelligence', 'Face Detection', 'Gaze Detection', 'Moondream2-2025-01-09']}
  >
    <Recipe.CodePreview 
      filename="gaze-detection-video.py"
      sourceUrl="https://github.com/vikhyat/moondream/tree/main/recipes/gaze-detection-video"
    >
      {`"""
Gaze Detection Video Processor using Moondream 2
------------------------------------------------
Read the README.md file for more information on how to use this script.
"""

import torch
import cv2
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Tuple, Optional
from contextlib import contextmanager

def initialize_model() -> Tuple[Optional[AutoModelForCausalLM], Optional[AutoTokenizer]]:
    """Initialize the Moondream 2 model with error handling."""
    try:
        print("Initializing Moondream 2 model...")
        model_id = "vikhyatk/moondream2"
        revision = "2025-01-09"  # Specify revision for stability
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            revision=revision,
            trust_remote_code=True,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map={"": device} if device == "cuda" else None
        )
        
        model = model.to(device).eval()
        tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
        return model, tokenizer
    except Exception as e:
        print(f"Error initializing model: {e}")
        return None, None

@contextmanager
def video_handler(input_path: str, output_path: str) -> Tuple[cv2.VideoCapture, cv2.VideoWriter]:
    """Context manager for handling video capture and writer."""
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        raise ValueError(f"Could not open video file: {input_path}")

    # Set up video writer with source properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))
    
    try:
        yield cap, out
    finally:
        cap.release()
        out.release()
        cv2.destroyAllWindows()`}
    </Recipe.CodePreview>

    <Recipe.Feature icon="ðŸ‘ï¸" title="Multi-Face Gaze Tracking">
      Detect and track gaze directions for multiple faces simultaneously in video frames.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¨" title="Dynamic Visualization">
      Real-time visualization with colored bounding boxes, gradient lines for gaze direction, and gaze target points.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¥" title="Video Processing">
      Support for common video formats (.mp4, .avi, .mov, .mkv) with progress tracking and batch processing capabilities.
    </Recipe.Feature>

    <Recipe.Feature icon="âš¡" title="GPU Acceleration">
      Optimized for GPU processing with automatic fallback to CPU when needed.
    </Recipe.Feature>

  </Recipe>

  ---

  <Recipe
    title="Interactive Image Analysis Demo"
    description="A Gradio-based web interface for real-time image analysis, featuring visual question answering and object detection with bounding box visualization."
    github="https://github.com/vikhyat/moondream/blob/main/gradio_demo.py"
    demo="https://youtu.be/T7sxvrJLJ14?feature=shared&t=1072"
    tags={['Gradio', 'Interactive UI', 'Object Detection', 'VQA']}
  >
    <Recipe.CodePreview 
      filename="gradio_demo.py"
      sourceUrl="https://github.com/vikhyat/moondream/blob/main/gradio_demo.py"
    >
      {`# Core setup
model_id = "vikhyatk/moondream2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
moondream = Moondream.from_pretrained(model_id).to(device)

# Create the interface

with gr.Blocks() as demo: gr.Markdown("# ðŸŒ” moondream") with gr.Row(): prompt = gr.Textbox(label="Input Prompt", value="Describe this image.") submit = gr.Button("Submit") with
gr.Row(): img = gr.Image(type="pil", label="Upload an Image") with gr.Column(): output = gr.Markdown(label="Response") ann = gr.Image(visible=False, label="Annotated Image")`}

</Recipe.CodePreview>

    <Recipe.Feature icon="âš¡ï¸" title="Streaming Responses">
      Get real-time text generation with token-by-token streaming for a more
      interactive experience.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¯" title="Automatic Annotations">
      Visualize object detection results with automatically generated bounding
      boxes overlaid on images.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸ”„" title="Flexible Runtime">
      Switch between CPU and GPU inference with automatic device detection
      and optimization.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸš€" title="Simple Deployment">
      Deploy as a standalone web app or integrate into existing Gradio
      applications with minimal configuration.
    </Recipe.Feature>

  </Recipe>

  ---
    <Recipe
    title="Jupyter Notebook Quick Start"
    description="Run Moondream in a Jupyter notebook or Google Colab environment with Google Drive integration for easy image analysis and experimentation."
    github="https://github.com/vikhyat/moondream/blob/main/examples/jupyter_notebook.ipynb"
    demo="https://colab.research.google.com/drive/1g88_P2Is8_dPqu-vX3D9VI80jJMjycUT?usp=sharing"
    tags={['Jupyter', 'Google Colab', 'Object Detection', 'VQA']}
  >
    <Recipe.CodePreview 
      filename="jupyter_notebook.ipynb"
      sourceUrl="https://colab.research.google.com/drive/1g88_P2Is8_dPqu-vX3D9VI80jJMjycUT?usp=sharing"
    >
      {`# Install and import dependencies
!pip install moondream
import moondream as md
from PIL import Image

# Download model weights
variant = '0.5b-int8'  # Lightweight model for basic usage
model_path = download_model(variant=variant)

# Initialize model
model = md.vl(model_path)

# Load and analyze image
image = Image.open("sample.jpg")
encoded_image = model.encode_image(image)

# Generate caption
caption = model.caption(encoded_image)
print(caption["caption"])

# Ask questions about the image
question = "What colors are prominent in this image?"
answer = model.query(encoded_image, question)
print(answer["answer"])`}
    </Recipe.CodePreview>

    <Recipe.Feature icon="âš¡ï¸" title="Multiple Model Options">
      Choose from different model variants optimized for your needs - from lightweight 0.5B models to full 2B versions.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¯" title="Google Drive Integration">
      Seamlessly load images and save results using Google Drive integration in Colab notebooks.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸ”„" title="Flexible Runtime Options">
      Run on CPU (Free Tier) or upgrade to GPU runtime for faster inference with T4/V100/A100 options.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸš€" title="Interactive Development">
      Experiment and iterate quickly with interactive Jupyter notebook environment and real-time output.
    </Recipe.Feature>

  </Recipe>

  ---
