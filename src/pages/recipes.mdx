import Head from 'next/head'
import Recipe from '../components/Recipe';

<Head>
  <title>Recipes - Moondream Documentation</title>
  <meta name="description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  {/* Open Graph */}
  <meta property="og:title" content="Moondream Recipes - Best Practices & Real-World Examples" />
  <meta property="og:description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://docs.moondream.ai/recipes" />
  
  
  {/* Twitter */}
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Moondream Recipes - Best Practices & Real-World Examples" />
  <meta name="twitter:description" content="Explore practical examples, optimization tips, and best practices for implementing Moondream vision AI in your applications. Learn from real-world use cases and implementation patterns." />
  
  
  {/* Schema.org for Google */}
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "TechArticle",
      "headline": "Moondream Vision AI Implementation Recipes",
      "description": "Collection of best practices and real-world examples for Moondream vision AI",
      "articleSection": "Technical Documentation",
      "keywords": "vision AI, implementation patterns, best practices, optimization, examples"
    })}
  </script>
</Head>

# Recipes

Explore real-world examples and implementation patterns using Moondream...

  <Recipe
    title="Gaze Detection Video Processor"
    description="A video processing application that uses Moondream 2 to detect faces and track gaze directions in videos, with real-time visualization of face detections and gaze directions using dynamic visual effects."
    github="https://github.com/vikhyat/moondream/tree/main/recipes/gaze-detection-video"
    tags={['Video Intelligence', 'Face Detection', 'Gaze Detection', 'Moondream2-2025-01-09']}
  >
    <Recipe.CodePreview 
      filename="gaze-detection-video.py"
      sourceUrl="https://github.com/vikhyat/moondream/tree/main/recipes/gaze-detection-video"
    >
      {`"""
Gaze Detection Video Processor using Moondream 2
------------------------------------------------
Read the README.md file for more information on how to use this script.
"""

import torch
import cv2
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Tuple, Optional
from contextlib import contextmanager

def initialize_model() -> Tuple[Optional[AutoModelForCausalLM], Optional[AutoTokenizer]]:
    """Initialize the Moondream 2 model with error handling."""
    try:
        print("Initializing Moondream 2 model...")
        model_id = "vikhyatk/moondream2"
        revision = "2025-01-09"  # Specify revision for stability
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            revision=revision,
            trust_remote_code=True,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map={"": device} if device == "cuda" else None
        )
        
        model = model.to(device).eval()
        tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
        return model, tokenizer
    except Exception as e:
        print(f"Error initializing model: {e}")
        return None, None

@contextmanager
def video_handler(input_path: str, output_path: str) -> Tuple[cv2.VideoCapture, cv2.VideoWriter]:
    """Context manager for handling video capture and writer."""
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        raise ValueError(f"Could not open video file: {input_path}")

    # Set up video writer with source properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))
    
    try:
        yield cap, out
    finally:
        cap.release()
        out.release()
        cv2.destroyAllWindows()`}
    </Recipe.CodePreview>

    <Recipe.Feature icon="ðŸ‘ï¸" title="Multi-Face Gaze Tracking">
      Detect and track gaze directions for multiple faces simultaneously in video frames.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¨" title="Dynamic Visualization">
      Real-time visualization with colored bounding boxes, gradient lines for gaze direction, and gaze target points.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¥" title="Video Processing">
      Support for common video formats (.mp4, .avi, .mov, .mkv) with progress tracking and batch processing capabilities.
    </Recipe.Feature>

    <Recipe.Feature icon="âš¡" title="GPU Acceleration">
      Optimized for GPU processing with automatic fallback to CPU when needed.
    </Recipe.Feature>

  </Recipe>

  ---

  <Recipe
    title="Interactive Image Analysis Demo"
    description="A Gradio-based web interface for real-time image analysis, featuring visual question answering and object detection with bounding box visualization."
    github="https://github.com/vikhyat/moondream/blob/main/gradio_demo.py"
    demo="https://youtu.be/T7sxvrJLJ14?feature=shared&t=1072"
    tags={['Gradio', 'Interactive UI', 'Object Detection', 'VQA']}
  >
    <Recipe.CodePreview 
      filename="gradio_demo.py"
      sourceUrl="https://github.com/vikhyat/moondream/blob/main/gradio_demo.py"
    >
      {`# Core setup
model_id = "vikhyatk/moondream2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
moondream = Moondream.from_pretrained(model_id).to(device)

# Create the interface

with gr.Blocks() as demo: gr.Markdown("# ðŸŒ” moondream") with gr.Row(): prompt = gr.Textbox(label="Input Prompt", value="Describe this image.") submit = gr.Button("Submit") with
gr.Row(): img = gr.Image(type="pil", label="Upload an Image") with gr.Column(): output = gr.Markdown(label="Response") ann = gr.Image(visible=False, label="Annotated Image")`}

</Recipe.CodePreview>

    <Recipe.Feature icon="âš¡ï¸" title="Streaming Responses">
      Get real-time text generation with token-by-token streaming for a more
      interactive experience.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¯" title="Automatic Annotations">
      Visualize object detection results with automatically generated bounding
      boxes overlaid on images.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸ”„" title="Flexible Runtime">
      Switch between CPU and GPU inference with automatic device detection
      and optimization.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸš€" title="Simple Deployment">
      Deploy as a standalone web app or integrate into existing Gradio
      applications with minimal configuration.
    </Recipe.Feature>

  </Recipe>

  ---
    <Recipe
    title="Jupyter Notebook Quick Start"
    description="Run Moondream in a Jupyter notebook or Google Colab environment with Google Drive integration for easy image analysis and experimentation."
    github="https://github.com/vikhyat/moondream/blob/main/examples/jupyter_notebook.ipynb"
    demo="https://colab.research.google.com/drive/1g88_P2Is8_dPqu-vX3D9VI80jJMjycUT?usp=sharing"
    tags={['Jupyter', 'Google Colab', 'Object Detection', 'VQA']}
  >
    <Recipe.CodePreview 
      filename="jupyter_notebook.ipynb"
      sourceUrl="https://colab.research.google.com/drive/1g88_P2Is8_dPqu-vX3D9VI80jJMjycUT?usp=sharing"
    >
      {`# Install and import dependencies
!pip install moondream
import moondream as md
from PIL import Image

# Download model weights
variant = '0.5b-int8'  # Lightweight model for basic usage
model_path = download_model(variant=variant)

# Initialize model
model = md.vl(model_path)

# Load and analyze image
image = Image.open("sample.jpg")
encoded_image = model.encode_image(image)

# Generate caption
caption = model.caption(encoded_image)
print(caption["caption"])

# Ask questions about the image
question = "What colors are prominent in this image?"
answer = model.query(encoded_image, question)
print(answer["answer"])`}
    </Recipe.CodePreview>

    <Recipe.Feature icon="âš¡ï¸" title="Multiple Model Options">
      Choose from different model variants optimized for your needs - from lightweight 0.5B models to full 2B versions.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸŽ¯" title="Google Drive Integration">
      Seamlessly load images and save results using Google Drive integration in Colab notebooks.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸ”„" title="Flexible Runtime Options">
      Run on CPU (Free Tier) or upgrade to GPU runtime for faster inference with T4/V100/A100 options.
    </Recipe.Feature>

    <Recipe.Feature icon="ðŸš€" title="Interactive Development">
      Experiment and iterate quickly with interactive Jupyter notebook environment and real-time output.
    </Recipe.Feature>

  </Recipe>
