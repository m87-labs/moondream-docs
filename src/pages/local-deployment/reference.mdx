import { Cards } from 'nextra/components'

# Moondream Local Deployment

Welcome to the Moondream local deployment documentation. Learn how to run Moondream's powerful vision-language capabilities directly on your own infrastructure.

## Installation

~~~bash filename="terminal"
pip install moondream
~~~

Download the model weights from our [Model Downloads](/model-downloads) page. While compressed models can be loaded directly, we recommend unzipping them first for faster loading times:

~~~bash filename="terminal"
gzip -d models/moondream-2b-int8.bin.gz
~~~

## Available Features
<Cards>
  <Cards.Card
    icon="🚀"
    title="Getting Started"
    href="/local-deployment/getting-started"
    description="Quick guide to authentication and basic usage of Moondream Cloud APIs"
  />
  <Cards.Card
    icon="💬"
    title="/query"
    href="/local-deployment/query"
    description="Ask natural language questions about images and receive detailed answers"
  />
  <Cards.Card
    icon="📝"
    title="/caption"
    href="/local-deployment/caption"
    description="Generate accurate and natural image captions"
  />
  <Cards.Card
    icon="🔍"
    title="/detect"
    href="/local-deployment/detect"
    description="Detect and locate objects in images"
  />
  <Cards.Card
    icon="📍"
    title="/point"
    href="/local-deployment/point"
    description="Get precise coordinate locations for objects in images"
  />
</Cards>

## Basic Usage

Initialize the model and use any of the available features:

~~~python filename="example.py"
from moondream import vl

# Load unzipped model for faster startup
model = vl("models/moondream-2b-int8.bin")

# Visual Q&A
answer = model.query(image, "What is in this image?")
print(answer['answer'])

# Image Captioning
caption = model.caption(image)
print(caption)

# Object Detection
objects = model.query(image, "What objects do you see?")
print(objects['answer'])

# Visual Pointing
coords = model.point(image, "point to the dog")
print(f"The dog is at coordinates: {coords}")
~~~

## Quick Reference

Here's a quick overview of all available features and their function signatures:

### Function Overview

| Feature | Method | Parameters |
| --- | --- | --- |
| Visual Q&A | `model.query()` | `image`, `question` |
| Image Captioning | `model.caption()` | `image` |
| Object Detection | `model.query()` | `image`, `"What objects do you see?"` |
| Visual Pointing | `model.point()` | `image`, `object_description` |

### Input Types

~~~python filename="input-types.py"
# Supported image input formats
image: Union[
    str,                # Path to image file
    PIL.Image.Image,    # PIL Image object
    numpy.ndarray,      # NumPy array (H,W,C) in RGB
    torch.Tensor        # PyTorch tensor (C,H,W) normalized
]

# Query parameters
question: str          # Natural language question
object_description: str # Description of object to locate
~~~

### Return Types

~~~python filename="return-types.py"
# Visual Q&A and Object Detection
type QueryResponse = {
    "answer": str  # Natural language response
}

# Image Captioning
type CaptionResponse = str  # Generated caption

# Visual Pointing
type PointResponse = List[Dict[str, float]]  # Coordinate points
~~~

## Performance Tips

- Use quantized models (e.g., `moondream-2b-int8.bin.gz`) for faster inference
- Keep images at reasonable resolutions (around 224x224 pixels)
- Batch process images when possible
- Use GPU acceleration when available